# ğŸš€ Hadoop Cluster Setup with Spark, Hive, and Pig  

## 1ï¸âƒ£ Start Docker ğŸ³  
- Open **Docker Desktop** (if not already running).  

## 2ï¸âƒ£ Rename User ğŸ”„  
- Open **WSL2**.  
- Navigate to the project folder:  
  ```bash
  cd HaMu
  ```
- Run the rename script:  
  ```bash
  python rename-owner.py
  ```
- Enter your new username (e.g., `hadoophuydeptrai`).  

## 3ï¸âƒ£ Build Docker Images ğŸ·ï¸  
- Move to the Hadoop config directory:  
  ```bash
  cd HaMu/config-hadoop
  ```
- Build the images:  
  ```bash
  docker compose build
  ```
  ![alt text](image-1.png)

## 4ï¸âƒ£ Start Containers ğŸš€  
- Return to the main directory:  
  ```bash
  cd ..
  ```
- Start all containers:  
  ```bash
  docker compose up -d
  ```
  ![alt text](image.png)
  ![alt text](image-3.png)

## 5ï¸âƒ£ Access the Master Container â›“ï¸  
  ```bash
  docker exec -it master /bin/bash
  ```
  ![alt text](image-2.png)
## 6ï¸âƒ£ Activate Hive ğŸ  
  ```bash
  schematool -initSchema -dbType derby
  ```
  ![alt text](image-6.png)

![alt text](image-5.png)
![alt text](image-7.png)
![alt text](image-8.png)
![alt text](image-9.png)
![alt text](image-4.png)
ğŸ’ª Now you can use your Hadoop Cluster with Spark, Hive, and Pig! ğŸ‰  

ğŸ”¹ **Thank you for using this setup!** ğŸ˜ƒ

