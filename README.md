# ğŸš€ HadoopSphere
A fully containerized **Hadoop, Spark, Hive, and Pig** environment for quick and efficient Big Data processing.  

[![All Contributors](https://img.shields.io/badge/all_contributors-1-orange.svg?style=flat-square)](#contributors-)  

## ğŸœ Table of Contents  
- ğŸ“š [My Story](#-my-story-feel-free-to-skip)  
- ğŸ‘¥ [Authors](#-authors)  
- âœ¨ [Features](#-features)  
- ğŸ”§ [Tech Stack](#-tech-stack)  
- ğŸ¥ [OS Support](#-os-support)  
- ğŸ“Œ [Prerequisites](#-prerequisites)  
- ğŸš€ [Installation Guide](#-installation-guide)  
- ğŸ”„ [Modify the Owner Name](#-modify-the-owner-name)  
- ğŸŒ [Interact with the Web UI](#-interact-with-the-web-ui)  
- âœ¨ [Contributors](#-contributors-)  
- ğŸ“ [Contact](#-contact)  

---

## ğŸ“š **My Story** *(feel free to skip)*  

Setting up a **Hadoop cluster** manually is frustrating, especially when integrating **Spark, Hive, and Pig**. My friend and I initially developed **HaMu** for a simple Hadoop deployment using Docker, and I extended it further by adding **Spark, Hive, and Pig** for full Big Data analytics support.  

ğŸ’¡ I hope **HadoopSphere** helps you quickly set up a Big Data environment for learning and development! ğŸš€  

---

## ğŸ‘¥ **Authors**  
- [@Quang Nguyen](https://github.com/DOCUTEE) *(Original Author)*  
- [@Huy Nguyen](https://github.com/huy-dataguy) *(Extended with Spark, Hive, Pig)*  

---

## âœ¨ **Features**  
ğŸ‘‰ Deploy a **multi-node** Hadoop-Spark-Hive-Pig cluster **with a single command**.  
ğŸ‘‰ Customize the number of slave nodes.  
ğŸ‘‰ Run **HDFS, YARN, Spark, Hive, and Pig** seamlessly inside Docker.  
ğŸ‘‰ Access Web UIs for monitoring Hadoop and Spark jobs.  
ğŸ‘‰ Use **Hive Metastore with Derby** (or integrate with MySQL/PostgreSQL).  
ğŸ‘‰ [Modify the cluster owner's name.](#-modify-the-owner-name)  

---

## ğŸ”§ **Tech Stack**  
- **Hadoop** (HDFS, YARN)  
- **Spark** (Standalone Mode)  
- **Apache Hive** (With Derby Metastore)  
- **Apache Pig**  
- **Docker** (Containerized Setup)  

---

## ğŸ¥ **OS Support**  
- ğŸª  **Windows** (via WSL2 or Docker Desktop)  
- ğŸ› **Linux** (Ubuntu, CentOS, Debian)  

---

## ğŸ“Œ **Prerequisites**  
- ğŸ’ª **Docker**  
- ğŸ“‚ **Basic Knowledge of Hadoop, Spark, Hive, Pig**  

---

## ğŸš€ **Installation Guide**  

### **Step 1: Clone the Repository**  
```sh
git clone https://github.com/huy-dataguy/HadoopSphere.git
cd HadoopShere
```

### **Step 2: Build Docker Images**  

#### ğŸª  **For Windows**  
```sh
.\windows\build-image.bat
```

#### ğŸ¬ **For Linux**  
```sh
./linux/build-image.sh
```

---

### **Step 3: Start the Cluster**  

#### ğŸª  **For Windows**  
```sh
.\windows\start-cluster.bat
```

#### ğŸ¬ **For Linux**  
```sh
./linux/start-cluster.sh
```

*By default, this will start a cluster with **1 master and 2 slaves**.*  

To start a cluster with **1 master and 5 slaves**:  
```sh
./linux/start-cluster.sh 6    # Linux  
.\windows\start-cluster.bat 6 # Windows  
```

---

### **Step 4: Verify the Installation**  

After starting the cluster, access the **master container** and run the following commands:  

ğŸ’¡ **Check HDFS Nodes**  
```sh
hdfs dfsadmin -report
```

ğŸ’¡ **Check Spark Cluster**  
```sh
spark-shell
```

ğŸ’¡ **Check Hive Metastore**  
```sh
schematool -dbType derby -info
```

ğŸ’¡ **Run a Pig Script**  
```sh
pig -x mapreduce
```
---

## ğŸ”„ **Modify the Owner Name**  
To change the cluster owner's username, run:  
```sh
python rename-owner.py
```

---

## ğŸŒ **Interact with the Web UI**  

ğŸ”¹ **Hadoop NameNode UI** â†’ [http://localhost:9870](http://localhost:9870)  
ğŸ”¹ **YARN Resource Manager UI** â†’ [http://localhost:8088](http://localhost:8088)  
ğŸ”¹ **Spark UI** â†’ [http://localhost:4040](http://localhost:4040)  

---

## âœ¨ **Contributors**  

| Contributor | Role |
|-------------|------|
| [@Quang Nguyen](https://github.com/DOCUTEE) | Original HaMu Creator |
| [@Huy Nguyen](https://github.com/huy-dataguy) | Spark, Hive, Pig Integration |

---

## ğŸ“ **Contact**  
ğŸ“§ Email: quochuy.working@gmail.com  

ğŸ’¬ Feel free to contribute and improve this project! ğŸš€























